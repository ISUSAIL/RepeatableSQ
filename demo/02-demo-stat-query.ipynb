{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d6c3cd2",
   "metadata": {},
   "source": [
    "# Repeatable Statistical Inference Demo - Statistical Query\n",
    "\n",
    "Standard Monte Carlo and Importance Sampling with different sample sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583f7850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All imports successful!\n"
     ]
    }
   ],
   "source": [
    "# Add parent directory to path for imports\n",
    "import sys\n",
    "import os\n",
    "sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Import required libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our implementations\n",
    "from distributions import (\n",
    "    Distribution01, BernoulliRare, BetaSkewed, \n",
    "    BimodalMixture, UniformSpike, TruncatedNormal\n",
    ")\n",
    "\n",
    "# Set up plotting style with larger fonts\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 14          # Increased from 12\n",
    "plt.rcParams['axes.labelsize'] = 16     # Axis labels\n",
    "plt.rcParams['axes.titlesize'] = 18     # Title size\n",
    "plt.rcParams['xtick.labelsize'] = 14    # X-axis tick labels\n",
    "plt.rcParams['ytick.labelsize'] = 14    # Y-axis tick labels\n",
    "plt.rcParams['legend.fontsize'] = 14    # Legend font size\n",
    "plt.rcParams['figure.titlesize'] = 20   # Figure title size\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "in94t73gjvp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating distributions...\n"
     ]
    }
   ],
   "source": [
    "# Create all 6 distributions for comprehensive testing\n",
    "print(\"Creating distributions...\")\n",
    "distributions = [\n",
    "    BernoulliRare(p=0.05),                     # 1. Rare events\n",
    "    BetaSkewed(alpha=0.5, beta=2.0),           # 2. Right-skewed\n",
    "    TruncatedNormal(mu=0.3, sigma=0.15),       # 3. Truncated normal\n",
    "    BimodalMixture(weight=0.3)                # 4. Bimodal mixture\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1076db3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_truncated_importance_sampling(samples, weights, truncation_threshold=50.0):\n",
    "    \"\"\"Apply truncated importance sampling with bias correction\"\"\"\n",
    "    # Truncate weights\n",
    "    truncated_weights = np.minimum(weights, truncation_threshold)\n",
    "    \n",
    "    # Compute bias correction factor\n",
    "    n_truncated = np.sum(weights > truncation_threshold)\n",
    "    if n_truncated > 0:\n",
    "        # Simple bias correction: adjust the normalization\n",
    "        total_raw_weight = np.sum(weights)\n",
    "        total_truncated_weight = np.sum(truncated_weights)\n",
    "        \n",
    "        if total_truncated_weight > 0:\n",
    "            # Bias correction factor to approximately restore the mean\n",
    "            bias_correction = total_raw_weight / total_truncated_weight\n",
    "            # Apply correction but cap it to prevent overcorrection\n",
    "            bias_correction = min(bias_correction, 2.0)  # Cap correction at 2x\n",
    "            truncated_weights *= bias_correction\n",
    "    \n",
    "    return truncated_weights\n",
    "\n",
    "def importance_sampling(target_dist: Distribution01, importance_dist: Distribution01, \n",
    "                                n_samples: int, seed: int = None) -> dict:\n",
    "    \"\"\"Enhanced importance sampling with proper truncated IS\"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    \n",
    "    # Sample from importance distribution\n",
    "    samples = importance_dist.sample(n_samples)\n",
    "    \n",
    "    # Compute importance weights\n",
    "    target_pdf = target_dist.pdf(samples)\n",
    "    importance_pdf = importance_dist.pdf(samples)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    weights = np.where(importance_pdf > 1e-12, target_pdf / importance_pdf, 0.0)\n",
    "    \n",
    "    # Apply proper truncated importance sampling with bias correction\n",
    "    weights = apply_truncated_importance_sampling(samples, weights, truncation_threshold=50.0)\n",
    "    \n",
    "    # Weighted average with proper normalization\n",
    "    if np.sum(weights) > 0:\n",
    "        normalized_weights = weights / np.sum(weights)\n",
    "        estimate = np.sum(samples * weights) / np.sum(weights)\n",
    "        \n",
    "        # Compute effective sample size\n",
    "        eff_sample_size = 1.0 / np.sum(normalized_weights**2)\n",
    "        \n",
    "        # Variance estimation for importance sampling\n",
    "        weighted_variance = np.sum(normalized_weights * (samples - estimate)**2)\n",
    "        standard_error = np.sqrt(weighted_variance / eff_sample_size)\n",
    "    else:\n",
    "        estimate = 0.0\n",
    "        eff_sample_size = 0.0\n",
    "        standard_error = float('inf')\n",
    "        normalized_weights = weights\n",
    "    \n",
    "    return {\n",
    "        'estimate': estimate,\n",
    "        'standard_error': standard_error,\n",
    "        'effective_sample_size': eff_sample_size,\n",
    "        'efficiency': eff_sample_size / n_samples if n_samples > 0 else 0.0,\n",
    "        'weight_stats': {\n",
    "            'min': np.min(weights),\n",
    "            'max': np.max(weights),\n",
    "            'mean': np.mean(weights),\n",
    "            'std': np.std(weights),\n",
    "            'cv': np.std(weights) / np.mean(weights) if np.mean(weights) > 0 else float('inf')\n",
    "        }\n",
    "    }\n",
    "\n",
    "def monte_carlo_sampling(distribution: Distribution01, n_samples: int, seed: int = None) -> dict:\n",
    "    \"\"\"Enhanced Monte Carlo with confidence intervals\"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    samples = distribution.sample(n_samples)\n",
    "    estimate = np.mean(samples)\n",
    "    standard_error = np.std(samples, ddof=1) / np.sqrt(n_samples)\n",
    "    \n",
    "    return {\n",
    "        'estimate': estimate,\n",
    "        'standard_error': standard_error,\n",
    "        'effective_sample_size': n_samples,\n",
    "        'efficiency': 1.0,\n",
    "        'weight_stats': {\n",
    "            'min': 1.0, 'max': 1.0, 'mean': 1.0, 'std': 0.0, 'cv': 0.0\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wli6e1raqk",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== MONTE CARLO vs IMPORTANCE SAMPLING COMPARISON ===\n",
      "Testing 6 distributions across 6 sample sizes\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š DISTRIBUTION: Bernoulli(0.050)\n",
      "   True mean: 0.050000\n",
      "   Importance distribution: Bernoulli(0.150)\n",
      "\n",
      "   n        MC Est     MC SE    IS Est     IS SE    Eff Gain IS Eff% \n",
      "   ----------------------------------------------------------------------\n",
      "   200      0.0400     0.0139   0.0519     0.0163   0.73     92.5    \n",
      "   500      0.0600     0.0106   0.0560     0.0107   0.98     92.0    \n",
      "   1000     0.0460     0.0066   0.0511     0.0072   0.84     92.6    \n",
      "   2000     0.0525     0.0050   0.0509     0.0051   0.95     92.6    \n",
      "   5000     0.0512     0.0031   0.0485     0.0032   0.98     92.9    \n",
      "   10000    0.0474     0.0021   0.0481     0.0022   0.92     93.0    \n",
      "\n",
      "\n",
      "ðŸ“Š DISTRIBUTION: Beta(0.5, 2.0)\n",
      "   True mean: 0.200000\n",
      "   No importance sampling benefit expected\n",
      "\n",
      "   n        MC Est     MC SE    IS Est     IS SE    Eff Gain IS Eff% \n",
      "   ----------------------------------------------------------------------\n",
      "   200      0.2132     0.0162   N/A        N/A      N/A      N/A     \n",
      "   500      0.1977     0.0093   N/A        N/A      N/A      N/A     \n",
      "   1000     0.1918     0.0065   N/A        N/A      N/A      N/A     \n",
      "   2000     0.1926     0.0046   N/A        N/A      N/A      N/A     \n",
      "   5000     0.1996     0.0030   N/A        N/A      N/A      N/A     \n",
      "   10000    0.1958     0.0021   N/A        N/A      N/A      N/A     \n",
      "\n",
      "\n",
      "ðŸ“Š DISTRIBUTION: Beta(2.0, 0.5)\n",
      "   True mean: 0.800000\n",
      "   No importance sampling benefit expected\n",
      "\n",
      "   n        MC Est     MC SE    IS Est     IS SE    Eff Gain IS Eff% \n",
      "   ----------------------------------------------------------------------\n",
      "   200      0.7890     0.0149   N/A        N/A      N/A      N/A     \n",
      "   500      0.8008     0.0093   N/A        N/A      N/A      N/A     \n",
      "   1000     0.8075     0.0065   N/A        N/A      N/A      N/A     \n",
      "   2000     0.8082     0.0046   N/A        N/A      N/A      N/A     \n",
      "   5000     0.8003     0.0030   N/A        N/A      N/A      N/A     \n",
      "   10000    0.8032     0.0021   N/A        N/A      N/A      N/A     \n",
      "\n",
      "\n",
      "ðŸ“Š DISTRIBUTION: TruncNorm(Î¼=0.30, Ïƒ=0.15)\n",
      "   True mean: 0.308286\n",
      "   Importance distribution: Beta(1.0, 1.0)\n",
      "\n",
      "   n        MC Est     MC SE    IS Est     IS SE    Eff Gain IS Eff% \n",
      "   ----------------------------------------------------------------------\n",
      "   200      0.3002     0.0099   0.2847     0.0143   0.48     52.7    \n",
      "   500      0.3067     0.0065   0.2898     0.0087   0.56     53.5    \n",
      "   1000     0.3035     0.0045   0.2967     0.0063   0.51     51.2    \n",
      "   2000     0.3077     0.0032   0.3046     0.0044   0.53     51.1    \n",
      "   5000     0.3066     0.0020   0.3075     0.0028   0.51     51.0    \n",
      "   10000    0.3054     0.0014   0.3086     0.0020   0.50     50.5    \n",
      "\n",
      "\n",
      "ðŸ“Š DISTRIBUTION: BiModal(w=0.30)\n",
      "   True mean: 0.620000\n",
      "   Importance distribution: Beta(1.0, 1.0)\n",
      "\n",
      "   n        MC Est     MC SE    IS Est     IS SE    Eff Gain IS Eff% \n",
      "   ----------------------------------------------------------------------\n",
      "   200      0.5916     0.0246   0.5357     0.0282   0.76     71.3    \n",
      "   500      0.6352     0.0154   0.5797     0.0200   0.60     58.6    \n",
      "   1000     0.6315     0.0111   0.5715     0.0220   0.25     26.8    \n",
      "   2000     0.6172     0.0078   0.6196     0.0153   0.26     25.9    \n",
      "   5000     0.6244     0.0049   0.6204     0.0092   0.28     28.8    \n",
      "   10000    0.6306     0.0035   0.6201     0.0067   0.26     26.8    \n",
      "\n",
      "\n",
      "ðŸ“Š DISTRIBUTION: UniformSpike(0.90, 0.10)\n",
      "   True mean: 0.540000\n",
      "   Importance distribution: Beta(2.0, 1.5)\n",
      "\n",
      "   n        MC Est     MC SE    IS Est     IS SE    Eff Gain IS Eff% \n",
      "   ----------------------------------------------------------------------\n",
      "   200      0.5280     0.0216   0.5395     0.0232   0.86     64.4    \n",
      "   500      0.5295     0.0134   0.5787     0.0153   0.77     59.0    \n",
      "   1000     0.5484     0.0096   0.5682     0.0114   0.71     57.8    \n",
      "   2000     0.5374     0.0067   0.5500     0.0090   0.55     51.7    \n",
      "   5000     0.5309     0.0042   0.5530     0.0057   0.54     52.1    \n",
      "   10000    0.5423     0.0030   0.5473     0.0044   0.46     44.8    \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive comparison across sample sizes and distributions\n",
    "sample_sizes = [200, 500, 1000, 2000, 5000, 10000]\n",
    "results_comprehensive = {}\n",
    "\n",
    "print(\"=== MONTE CARLO vs IMPORTANCE SAMPLING COMPARISON ===\")\n",
    "print(f\"Testing {len(distributions)} distributions across {len(sample_sizes)} sample sizes\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for dist in distributions:\n",
    "    print(f\"\\nðŸ“Š DISTRIBUTION: {dist.name}\")\n",
    "    print(f\"   True mean: {dist.true_mean():.6f}\")\n",
    "    \n",
    "    importance_dist = dist.suggest_importance_distribution()\n",
    "    use_importance = importance_dist.name != dist.name\n",
    "    \n",
    "    if use_importance:\n",
    "        print(f\"   Importance distribution: {importance_dist.name}\")\n",
    "    else:\n",
    "        print(\"   No importance sampling benefit expected\")\n",
    "    \n",
    "    dist_results = {\n",
    "        'monte_carlo': {},\n",
    "        'importance_sampling': {} if use_importance else None,\n",
    "        'true_mean': dist.true_mean()\n",
    "    }\n",
    "    \n",
    "    # Test each sample size\n",
    "    print(f\"\\n   {'n':<8} {'MC Est':<10} {'MC SE':<8} {'IS Est':<10} {'IS SE':<8} {'Eff Gain':<8} {'IS Eff%':<8}\")\n",
    "    print(\"   \" + \"-\" * 70)\n",
    "    \n",
    "    for n in sample_sizes:\n",
    "        # Monte Carlo\n",
    "        mc_result = monte_carlo_sampling(dist, n, seed=42)\n",
    "        mc_error = abs(mc_result['estimate'] - dist.true_mean())\n",
    "        dist_results['monte_carlo'][n] = {\n",
    "            'estimate': mc_result['estimate'],\n",
    "            'standard_error': mc_result['standard_error'],\n",
    "            'error': mc_error,\n",
    "            'relative_error': mc_error / dist.true_mean() if dist.true_mean() != 0 else mc_error\n",
    "        }\n",
    "        \n",
    "        # Importance Sampling\n",
    "        if use_importance:\n",
    "            is_result = importance_sampling(dist, importance_dist, n, seed=42)\n",
    "            is_error = abs(is_result['estimate'] - dist.true_mean())\n",
    "            dist_results['importance_sampling'][n] = {\n",
    "                'estimate': is_result['estimate'],\n",
    "                'standard_error': is_result['standard_error'],\n",
    "                'error': is_error,\n",
    "                'relative_error': is_error / dist.true_mean() if dist.true_mean() != 0 else is_error,\n",
    "                'efficiency': is_result['efficiency'],\n",
    "                'effective_sample_size': is_result['effective_sample_size']\n",
    "            }\n",
    "            \n",
    "            # Efficiency metrics\n",
    "            variance_ratio = (mc_result['standard_error']**2) / (is_result['standard_error']**2) if is_result['standard_error'] > 0 else float('inf')\n",
    "            \n",
    "            print(f\"   {n:<8} {mc_result['estimate']:<10.4f} {mc_result['standard_error']:<8.4f} \"\n",
    "                  f\"{is_result['estimate']:<10.4f} {is_result['standard_error']:<8.4f} \"\n",
    "                  f\"{variance_ratio:<8.2f} {100*is_result['efficiency']:<8.1f}\")\n",
    "        else:\n",
    "            print(f\"   {n:<8} {mc_result['estimate']:<10.4f} {mc_result['standard_error']:<8.4f} \"\n",
    "                  f\"{'N/A':<10} {'N/A':<8} {'N/A':<8} {'N/A':<8}\")\n",
    "    \n",
    "    results_comprehensive[dist.name] = dist_results\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sim2real_reward",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
